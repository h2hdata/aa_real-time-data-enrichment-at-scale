{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA MUNGING, EXPLORATION AND ENRICHMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# FOR NER\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk import tree\n",
    "from geotext import GeoText\n",
    "\n",
    "from nltk.corpus import stopwords as stopwords_nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " DATA MUNGING\n",
    "---------------\n",
    "\"\"\"\n",
    "\n",
    "class NerCore:\n",
    "\tdef __init__(self, sentence):\n",
    "\t\tself.sentence = sentence\n",
    "\n",
    "\tdef __preprocessing(self, lang):\n",
    "\t\tsentences = sent_tokenize(self.sentence)\n",
    "\t\tsentences = [word_tokenize(sent) for sent in sentences]\n",
    "\t\tsentences = [pos_tag(sent, lang=lang) for sent in sentences]\n",
    "\t\treturn sentences\n",
    "\n",
    "\tdef extract_names(self, lang):\n",
    "\t\tnames = []\n",
    "\t\tsentences = self.__preprocessing(lang)\n",
    "\t\tfor tagged_sentence in sentences:\n",
    "\t\t\tfor chunk in ne_chunk(tagged_sentence):\n",
    "\t\t\t\tif isinstance(chunk, tree.Tree):\n",
    "\t\t\t\t\tif chunk.label() == 'PERSON':\n",
    "\t\t\t\t\t\tnames.append(' '.join([c[0] for c in chunk]))\n",
    "\t\treturn names\n",
    "\n",
    "\tdef extract_location(self, lang):\n",
    "\t\tlocations = []\n",
    "\t\tsentences = self.__preprocessing(lang)\n",
    "\t\tfor tagged_sentence in sentences:\n",
    "\t\t\tfor chunk in ne_chunk(tagged_sentence):\n",
    "\t\t\t\tif isinstance(chunk, tree.Tree):\n",
    "\t\t\t\t\tif chunk.label == 'LOCATION':\n",
    "\t\t\t\t\t\tlocations.append(' '.join([c[0] for c in chunk]))\n",
    "\n",
    "\t\tcities = GeoText(self.sentence).cities\n",
    "\t\tfor city in cities:\n",
    "\t\t\tlocations.append(''.join([c[0] for c in city]))\n",
    "\t\tnationalities = GeoText(self.sentence).nationalities\n",
    "\t\tfor national in nationalities:\n",
    "\t\t\tlocations.append(''.join([n[0] for n in national]))\n",
    "\t\tcountries = GeoText(self.sentence).countries\n",
    "\t\tfor country in countries:\n",
    "\t\t\tlocations.append(''.join([c[0] for c in country]))\n",
    "\t\treturn locations\n",
    "\n",
    "\tdef extract_date_time(self):\n",
    "\t\tdateTimes = []\n",
    "\t\tgrammar = r\"DATE: {<NNP><CD>}\"\n",
    "\t\tparser = nltk.RegexpParser(grammar)\n",
    "\n",
    "\t\tphrase_tagger = nltk.pos_tag(word_tokenize(self.sentence))\n",
    "\t\tphrase_chunk = nltk.ne_chunk(phrase_tagger)\n",
    "\n",
    "\t\tphrase_date = parser.parse(phrase_chunk)\n",
    "\t\tfor word in phrase_date:\n",
    "\t\t\tif isinstance(word, nltk.tree.Tree) and word.label() == 'DATE':\n",
    "\t\t\t\tdateTimes.append(' '.join([w[0] for w in word]))\n",
    "\t\treturn dateTimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"Twitter Demo\")\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 10) #10 is the batch interval in seconds\n",
    "IP = \"localhost\"\n",
    "Port = 5555\n",
    "lines = ssc.socketTextStream(IP, Port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " DATA ENRICHMENT\n",
    "-----------------\n",
    "\"\"\"\n",
    "\n",
    "def searchGeolocation(sentence):\n",
    "\n",
    "\treg = '(geo=\\()( )*[0-9]+( )*,( )*[0-9]+( )*\\)'\n",
    "\n",
    "\tm = re.search(reg,sentence)\n",
    "\n",
    "\tif m:\n",
    "\t\tfound = m.group(0)\n",
    "\n",
    "\t\tout = found[5:-1].split(',')\n",
    "\n",
    "\t\treq = [int(i) for i in out ]\n",
    "\n",
    "\t\treturn req\n",
    "\n",
    "\telse:\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "def stateSearch(sentence):\n",
    "\n",
    "\treg = '([A-Z]{2})'\n",
    "\n",
    "\tm = re.search(reg,sentence)\n",
    "\n",
    "\tif m:\n",
    "\t\tfound = m.group(0)\n",
    "\n",
    "\t\treturn found\n",
    "\n",
    "\telse:\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "def zipcodeExtracter(sentence):\n",
    "\n",
    "\treg = re.compile('^.*(?P<zipcode>\\d{5}).*$')\n",
    "\n",
    "\ttry:\n",
    "\t\tmatch = reg.match(sentence)\n",
    "\n",
    "\t\treturn match.groupdict()['zipcode']\n",
    "\texcept:\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "def get_result(sentence):\n",
    "\tout = []\n",
    "\tfor word in sentence:\n",
    "\t\tout.append(word)\n",
    "\treturn out\n",
    "\n",
    "def map_tweets(tweet):\n",
    "\tjson_tweet = json.loads(tweet)\n",
    "    \n",
    "\tif json_tweet.has_key('lang'): # When the lang key was not present it caused issues\n",
    "        \n",
    "\t\tif json_tweet['lang'] == 'en':\n",
    "            \n",
    "\t\t\ttweet = json_tweet['text']\n",
    "\t\t\tname = json_tweet['user']['name']\n",
    "\t\t\tuserid = json_tweet['user']['id']\n",
    "            \n",
    "\t\t\treferance = pd.read_csv('../Data/us_codes.csv')\n",
    "\t\t\tstatelist = referance['State'].tolist()\n",
    "\t\t\tstateabbrv = referance['State Abbreviation'].tolist()\n",
    "\t\t\tcountrylist = referance['County'].tolist()\n",
    "\t\t\tplacelist = referance['Place Name'].tolist()\n",
    "\t\t\tziplist = referance['Zip Code'].tolist()\n",
    "\t\t\tlatlist = referance['Latitude'].tolist()\n",
    "\t\t\tlonglist = referance['Longitude'].tolist()\n",
    "\t\t\tstate=None\n",
    "            \n",
    "\t\t\ttry:\n",
    "\t\t\t\tout = searchGeolocation(tweet)\n",
    "\t\t\t\tif out != 0:\n",
    "\t\t\t\t\tlat = out[0]\n",
    "\t\t\t\t\tlongt = out[1]\n",
    "\t\t\t\t\tif lat in latlist and longt in longlist:\n",
    "\t\t\t\t\t\tindex = latlist.index(lat)\n",
    "\t\t\t\t\t\tstate = statelist[index]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tstate = None\n",
    "\n",
    "\t\t\ttokenizer = RegexpTokenizer(r'\\w+')\n",
    "\t\t\tsentence = tokenizer.tokenize(tweet)\n",
    "\t\t\tsentence = ' '.join(sentence)\n",
    "\t\t\tgood_sentence = sentence # Use this for Semantic Analysis\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\touter = stateSearch(sentence)\n",
    "\t\t\t\tif outer != 0:\n",
    "\t\t\t\t\tif outer in stateabbrv:\n",
    "\t\t\t\t\t\tindex = stateabbrv.index(outer)\n",
    "\t\t\t\t\t\tstate = statelist[index]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tstate = None\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\tner = NerCore(sentence)\n",
    "\t\t\t\tlocations = ner.extract_location(\"eng\")\n",
    "\t\t\t\treq = get_result(locations)\n",
    "                \n",
    "\t\t\t\tif len(req) > 0:\n",
    "\t\t\t\t\tloc = max(set(req),keys=lst.count)\n",
    "\t\t\t\t\tif loc in countrylist:\n",
    "\t\t\t\t\t\tindex = countrylist.index(loc)\n",
    "\t\t\t\t\t\tstate = statelist[index] \n",
    "\t\t\t\t\telif loc in placelist:\n",
    "\t\t\t\t\t\tindex = placelist.index(loc)\n",
    "\t\t\t\t\t\tstate = statelist[index]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tstate = None\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\touter = str(zipcodeExtracter(sentence))\n",
    "\t\t\t\tif  outer!= '0':\n",
    "\t\t\t\t\t\tziplist = map(lambda x: str(x),ziplist)\n",
    "\t\t\t\t\t\tif outer in ziplist:\n",
    "\t\t\t\t\t\t\tindex = ziplist.index(outer)\n",
    "\t\t\t\t\t\t\tstate = statelist[index]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tstate = None\n",
    "\t\t\t\n",
    "\t\t\tif state is None:\n",
    "\t\t\t\tstate= '-----'\n",
    "\n",
    "\t\t\t#Semantic Hobby Search\n",
    "\t\t\thobby_referance = pd.read_csv('../Data/Hobbies.csv')\n",
    "\t\t\thobbieslist = hobby_referance['Hobbies'].tolist()\n",
    "\n",
    "\t\t\tfrom stemming.porter2 import stem\n",
    "\n",
    "\t\t\tsentence = [stem(word) for word in sentence.split(' ')]\n",
    "\t\t\thobbieslist = [stem(word) for word in hobbieslist]\n",
    "\n",
    "\t\t\thobbieslist = list(set(sentence).intersection(set(hobbieslist)))\n",
    "\n",
    "\t\t\tif hobbieslist:\n",
    "\t\t\t\treturn [str(userid), str(state), str(hobbieslist).strip('[').strip(']')]\n",
    "\t\t\t\n",
    "\t\t\treturn [str(userid), str(state), '-------']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-512bb35e350e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeachRDD\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Output/Raw_output/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/streaming/context.pyc\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# When your DStream in Spark receives data, it creates an RDD every batch interval.\n",
    "# We use coalesce(1) to be sure that the final filtered RDD has only one partition,\n",
    "# so that we have only one resulting part-00000 file in the directory.\n",
    "# The method saveAsTextFile() should really be re-named saveInDirectory(),\n",
    "# because that is the name of the directory in which the final part-00000 file is saved.\n",
    "# We use time.time() to make sure there is always a newly created directory, otherwise\n",
    "# it will throw an Exception\n",
    "\n",
    "lines.foreachRDD( lambda rdd: rdd.map(map_tweets).coalesce(1).saveAsTextFile('../Output/Raw_output/'+str(time.time())) )\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
